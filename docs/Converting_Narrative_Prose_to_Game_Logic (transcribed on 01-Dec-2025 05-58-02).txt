Welcome to the deep dive today. We're looking at something Pretty fundamental. We're tackling a classic dilemma in well in any kind of immersive development How do you take the messy beautiful high-end trippy world of human narrative? I mean think of a 500 page novel right with all its subtext and context and translate that into crisp structured Machine-readable logic for a game that really is the multi trillion dollar question isn't it especially for anyone building these massive Lore-rich dynamic worlds and the tools we've been using historically Well, they're just not built for it. You're talking about things like Jason primarily Jason and look bless its heart Jason is great for what it was designed for but it's a human-centric format that is just It's bloated. It's full of syntactic noise all those quotes the commas the colons Exactly, it's so verbose and when you start to scale that up when you have an entire world Bible Thousands of pages of lore and rules that has to be parsed constantly by an engine or even worse by these expensive large language models That redundancy isn't just annoying anymore. It's a bottleneck a huge Operational bottleneck it chews up memory It slows down load times and it burns through your LLM token budget Just parsing punctuation instead of actual meaning if the very foundation of your world is creating friction You're never gonna get the speed or stability you need so this is what brings us to today's deep dive We're looking at the Ingen projects really radical solution to this They basically just they threw out the old formats and built a new two-part stack from scratch Right and this stack is made of two key components You've got the destination format which is the super efficient Xeon binary format or ZO NB And then you have the source format the thing that feeds it which they call the ZW or ZEagle Waga semantic notation It's designed to be AI native from the ground up So our mission today is to really unpack the specs of these formats down to the byte level But more than that we're going to explore their whole pipeline the E MEP Which takes raw pros and converts it with full traceability Into this compact binary memory that's ready to run inside an engine like Godot It's a journey from like linguistic philosophy straight into production code. So, okay, let's unpack this We've got these two formats and they're designed to work together almost like a compiler. That's a great way to think about it Let's start the end then let's start with ZO and B because if we understand where the data needs to end up It really explains why the source format ZW had to be invented in the first place. Absolutely ZO and B is the foundation. It's the final hardened tight save container for everything All the world memory the game state the narrative structures It all lives here and it's designed purely for the machine not for us not at all Yeah, it prioritizes efficiency speed and safety above all else when a game engine loads memory It has to be able to trust it, you know implicitly ZO B is built to provide that trust and the spec itself the v1.0 spec is just it's incredibly rigorous They call it ZO N which is ZW object notation binary format and the first thing they stress is that it's platform agnostic Right, it doesn't care if you're running on a PC a console a phone. It just cares about pure predictable serialization And it's all built on for What they call non-negotiable pillars. Okay, what are they? So you have platform independence, which we just mentioned you have versioning which is crucial for future proofing efficiency from its compact binary form and the most important one type safety Every single value in a ZOMB file has an explicit type marker no guessing allowed Let's dig into that strictness the spec is like a checklist if you build an encoder your file must start with the four-byte magic header ZOMB right that immediately tells the loader what it's dealing with and right after that you have the version bite for now It's fixed at zero by zero one Okay, and then there's this really interesting technical choice the spec demands that all integers must use big end-dean encoding Why that specifically what kind of headache does enforcing that standard solve that is a deliberate and A very smart choice for platform independence for anyone listening ending this is just about bite order How a computer stores numbers bigger than one bite right little endian versus big exactly a lot of modern chips are little India but big endian has historically been the standard for networks and file formats because it's just It's more predictable across different systems By forcing big endian they guarantee the good old order reads those numbers correctly every single time no matter what hardware It's on so it's a way to kill a whole class of silent data corruption bugs before they can even happen a preemptive strike Yeah, absolutely, and that same thinking applies to the versioning right you said versioning is safety It is the current version zero by zero one defines this core structure But the spic is already looking ahead It says any decoder must reject the file with a version bite it doesn't recognize so an old game engine won't crash Trying to load a new data file. It'll just safely reject it And they've already planned out future versions zero by zero two might add compression flags zero by zero three could add new dated types Like 64-bit integers for you know huge numbers it builds instability for the long haul Okay, so let's get back to type safety because this feels like the real killer feature compared to more dynamic formats You said every value gets a single byte type marker. Yes, and the zero by ten to zero by sixteen range It's the heart of the whole structure You have zero by ten for an integer zero by eleven for a Boolean zero by twelve for a string and so on for a raise Dictionary's floats and null so when the Godot engine hits one of these bites it knows instantly what's coming next It doesn't have to guess or infer it see zero by ten it knows the next few bytes or an integer It see zero by fifteen it knows it's a float. So what's the actual performance gain there? I mean gd script is dynamically typed So how does this strict binary typing help compared to just parsing g-sun where everything comes back as a sort of generic variant? Well, the games are not trivial at all You could be shaving Milliseconds off every data structure you load and that really adds up when a gd script json parser reads a value it gets this generic Variant type the engine then has to spend cycles checking. Okay, is this an integer? Is it a float all these runtime checks? Exactly zo in b just eliminates that the hand optimized loader Sees the type marker it knows what's coming and it maps it directly into the correct gd script type No expensive type inference needed it just makes the whole memory assignment process faster and Way less error prone that attention to detail is something I mean the spec even calls out having to handle the Boolean type before the integer type in the logic because in Python a Boolean is technically a subclass of integer Right they had to account for that to guarantee the purity of the format Okay, that focus on purity brings us to the real innovation here the thing that makes the compression work the field ID mapping Yes, this is the secret sauce. This is where zo and b says okay We are done with human readability at the binary level and in exchange it gets this incredible efficiency So instead of writing out a long field name like description over and over you just write a single byte a field ID It's like a primary key in a database you don't repeat the whole definition you just stored the ID precisely The full string description only exists in one place the decoder's mapping table the schema It's never in the data file itself and the spec allocates these IDs really strictly 0 by 0 1 to 0 by 0 f are for core system fields like type name id And then the higher numbers are for more common game fields like time or created exactly Okay, so let's talk about the trade-off here the size savings are obvious But it's rigid if you ever decided that 0 by 0 2 shouldn't mean name anymore that would be Oh, it'd break everything so it doesn't giving up that human readability make debugging and maintenance way harder That's a crucial point and it really highlights the difference between an authoring format and a runtime format Zio and b is pure runtime all that complexity of schema management and making sure 0 by 0 2 is always named That gets pushed up into the tool chain into the zw empire editor We'll talk about later right the developers made a bet They bet that the huge size reduction in the absolute type safety were worth the cost of needing a specialized tool to debug the memory You're never meant to look at the raw hex and they guarantee that the python implementation and the gd script implementation Produce byte identical output for the same input that round trip fidelity that determinism is everything It's what lets the system trust the data as it moves from the high level python world down to the gdow runtime zio and b creates this incredibly predictable artifact that the engine can just ingest and run with No questions asked. Okay, so zio and b is the destination It solves for binary speed and safety, but now we have to talk about the other half of this equation We need to talk about zw the format designed to solve for source clarity and authoring efficiency Right and zw the zigal waga semantic notation is it's a whole philosophy If zio and b is the machine language zw is the source language for both humans and ai And like you said before it's fundamentally an anti jason project They explicitly reject what they call syntactic noise. Yes The whole philosophy is built on this pretty radical idea that a data format should be designed for the ai first And human readability should just be a happy side effect not the primary goal that burdens formats like jason And they call this semantic compression. What does that actually mean beyond just you know saving a few characters semantic compression is about maximizing the information density of every single token In jason, you have Quotes colons commas. Mm-hmm. None of those actually carry meaning about the data itself. They just define structure They're just punctuation. They're just punctuation. Yeah semantic compression aims to get rid of every single one of those non meaning carrying characters While keeping the structure perfectly intact The goal is to make the world's source code look more like Like structured language and less like programming boilerplate which leads to these incredibly sparse syntax rules. Oh, yeah It's the anti jason manifesto blocks use braces with bare word keys So it's just key value no quotes on the keys that alone is a huge saving to raise use brackets value one value two And they're absolutely no commas the white space is the delimiter just the white space. It's incredibly lean I think my favorite feature just from an authoring perspective is how the parser handles repeated keys If i'm defining a room and it has five items I don't have to manually create an items array I just write a more sur five times and the parser just gets it It sees the repeated item key and automatically merges them all into a list for you That's essential for writing game logic fast Imagine defining room exits. You don't want to type exits direction north direction south Just want to type exit direction north and then exit directions out and the parser understands that means collect all these exits into an array The devlogs show that getting that specific merge logic, right? Was one of the trickiest products of the whole thing Okay, so let's push on that trade-off again By sacrificing all that punctuation Doesn't it make the learning curve for say a modern much steeper if you miss a single space The whole structure could just fall apart. It absolutely raises the bar for the tools But not necessarily for the authors themselves if they have the right interface This is why the zw empire editor was so critical The engand team knew that humans need visual cues So the editor abstracts away the raw syntax. It gives you templates and real-time validation. So for a modern Yes, there's a small learning curve But the payoff is a much cleaner faster way to define data that almost looks like structured english The focus is efficiency at the source not necessarily ease of typing in night pad and the numbers seem to back that up I mean the compression metrics are pretty staggering. They're the proof of concept The analysis showed 11 to 20 fewer tokens than json, but the character savings And that's the headline up to 24.7% fewer characters and in some aggressive tests It went up to 47.3 percent almost half almost half the characters are just gone It proves that for purely declarative data like this door is open All of jason's overhead is just unnecessary noise and the implication for ai is maybe even bigger than for the game engine itself The project uses byte pair encoding bpe for its language models Can you explain why saving characters translates into such huge speed gains for an ai? certainly So bpe is how lm's break text into chunks into tokens to process it In json common character sequences like a to worry off Often become their own separate tokens. They add to the total token count which adds to the computational load So the ai is wasting processing power on punctuation It is cw gets rid of those sequences entirely So the density of meaningful characters inside each token goes way up That 47.3 percent character saving translates to what the team estimates is a 1.5 x to 2.0 x token compression Wow, so the ai agents in the engine the tray agent or mr. Lore they can process a scene or a character's memory using almost half the number of tokens They would need if it was stored in jason and processing scenes twice as fast means well lower operational costs for one llms are not cheap to run That's the first thing but more importantly. It means higher reliability When an llm has less structural noise to parse it can dedicate its full capacity to the actual semantic meaning less confusion means fewer hallucinations Exactly the ai is less likely to misinterpret the data structure and generate something incoherent Zw uses basically a linguistic choice designed to give the ai a cleaner mental model of the world its semantic liberation Okay, so we have zw for lean authoring z o and b for fast execution But these are just specs on paper if the toolchain connecting them isn't rock solid Right the whole architecture lives or dies on the pipeline that turns zw text into zomi binary and that validation process was Uh, let's just say it was meticulous and the first big hurdle was just getting the zw parser itself to work correctly Yes Zw's minimalist syntax its reliance on white space It's actually incredibly hard to parse correctly compared to something rigid like jason and they hit some classic really frustrating bugs right away The dev logs have this great story about the tokenizer failing. What exactly went wrong there? It was such a subtle error, but it cost them a lot of time the initial rejects pattern They were using in the tokenizer was it was too greedy didn't correctly see a closing brace Is its own token if it was right after a value so instead of parsing object and then it would bundle them together You get these garbage tokens like object or in one of the test cases chest. Oh, that's a nightmare to debug You're looking at your game logic failing and the root cause is a stray brace that got stuck to a word Exactly they had to do surgical correction on the rejects to make sure those special characters were always treated as their own delimiters It was one of those Two hours of work saves two weeks of debugging moments So once they stabilized that parser and got the array merging to work They focused on proving the whole chain was bulletproof from zw text to a python dictionary And then finally to the dot zombie binary right and confirming that round trip fidelity was a huge milestone They take complex data pack it into zomi immediately unpack it and confirm the resulting dictionary was byte for byte identical to the original That build confidence in the whole python toolchain and to make this whole process usable for actual humans They built the g.o the zw empire editor. This thing was basically their architectural sandbox It was way more than just a text editor. It had this optimized split pane interface On one side you had template quick buttons So an author could just click to insert a valid structure for a container an mpc a room So it helps manage that stupor learning curve we were talking about it gives you instant scaffolding And then the output side was key for validation. It had four tabs parsed validation stats and a view of the final zw mb binary And that stats panel was the constant proof of concept. That's where they could see the compression happening in real time They had test cases where a structure that was 376 characters in jason got packed down into just 110 bytes of zomb Wait, so we went from a 47 percent character saving with zw versus jason To now a 70 percent size compression from the jason equivalent to the final zomb binary over 70 percent Yeah, where do those extra savings come from mostly from two places First the field eddy mapping we talked about you're not storing description You're storing a single byte and second just pure binary encoding the number 4096 takes four characters in text But it can be stored in just two bytes in binary zw mb just sheds all that ascii overhead Okay, so the editor proved the pipeline works But the final final test was getting those zomb files loaded and running natively inside godot And this is where the core architectural principle for runtime speed comes in jason is never involved on the godot side ever So it's not unpacking zonb back to jason text and then parsing that no absolutely not the flow is strictly Zw text gets turned into a python dictionary It gets packed into a zo and d binary and the gd script loader reads that binary directly into a gd script dictionary By passing all the expensive text parsing the gd script loader must be tiny and super efficient then it is It's basically just a smart byte reader Because of the explicit type markers and the field id's it knows exactly what to do they read 0 by 15 It knows the next chunk of bytes as a float And that instant type recognition in a dynamic language like gd script is key They loaded complex containers zork style mazes and it all just worked fast type save and stable So we validated the formats we validated the tool chain now we get to the most ambitious part of this whole project The part that feels like like data alchemy Automatically converting raw pros like a chapter from a novel into structured zw logic It really is the moonshot And to do it they had to throw out a lot of the assumptions of traditional natural language processing or nlp Why does traditional nlp fail here because human communication has two layers There's the explicit knowledge what is literally written on the page And then there's the implicit meaning everything we infer from context and shared understanding If your game engine only understands the explicit facts, it's missing half the world Which led them to create e me the engan meta extraction plan This is their eight layer framework for taking untrusted unstructured text and turning it into clean executable Zw right and we should probably walk through their layers because they're fascinating It all starts with layer one, which is just input validation basic cleanup You know normalizing white space checking character encoding just getting the text ready And then layer two is where the magic starts pass one the explicit reconstruction. This is the key innovation Pass one does not guess it does not infer It performs what they call semantic preserving surgery on the text Surgery so it's making things that are implied by convention explicit for the machine What kind of things it ensures every single line has a definitive type in actor It normalizes all the dialogue quotes and crucially it will insert an explicit speaker name tag Even if the speaker wasn't named right next to the quote it classifies every line is this narration is this dialogue Is this an internal monologue the output is what they call the canonical layer The text we can absolutely trust. Okay, so once you have that trustworthy canonical text You move to layer three, which is pass two the implicit reasoning pass. This is the detective work It's an isolated pass that uses heuristics and domain knowledge to infer all the missing semantic data So this is where it's building meaning around the text. What are some of the hardest problems pass two has to solve? Well, the speaker inference is a huge one and author will often write a line of dialogue and not tell you who said it until the next sentence We go olife observed pass two has to look ahead and behind to correctly attribute that quote to olife And pronoun resolution that's famously difficult notoriously pass two uses a stateful last actor tracker If the last person mentioned was sergeant dachs and the next sentence says he lifted the gun Pass two links he backed to dachs I read about a test case where this went wrong with a non-human actor the story about the dog vixen Yes, that's a perfect example of the complexity in the test scene sergeant dachs was petting his dog vixen The next line was she stood up and paced the perimeter The system's last actor logic kept trying to link she to the last known female human character Who wasn't even in the room because it prioritized humans over animals exactly They had to refine the heuristics to include things like proximity and grammatical probability Basically teaching the model that a dog standing up is a much more likely event than a character Teleporting in from offstage. That's the level of detail that makes this a real game AI And pass two also does action and emotion mapping. Yes, that's the core plus plus part It uses these big keyword lists to detect universal emotions like gratitude or fear But it also detects the highly specific in-world actions for the ingame universe What's an example of a domain action like real manipulation? What is that analogous to think of it like a magic system? In a fantasy game, you wouldn't just detect attack. You'd want to detect fireball casting Here real manipulation might be a specific psychic ability Triggered by keywords in the text like coalesced or repatterned or something like chemical fear, which isn't just regular fear It's a specific game mechanic triggered by a toxin pass two tags all these game relevant atoms. Okay, so that's a lot of extraction What comes next in the pipeline layer four is schema validation? It's a quality check it takes everything from pass one and pass two and checks it against the known world schema Does this actor exist? Is this action possible? It catches inconsistencies early then layer five is the zw Semantic packaging right the merger pass. This is where all gets assembled into the final ZW blocks the canonical text from pass one is combined with the metadata from pass two Into a structure like scene 123 and from there it goes to layer six zio and j synthesis You mentioned this isn't a file format right? It's the final in-memory dictionary structure just before it gets packed and the crucial thing it does is Segregate the inferred knowledge all those bits from pass two the pronoun resolutions the speaker inferences They all get put into a dedicated inferred field. Why is that separation so important? Why not just merge it all together? traceability and trust This way the engine always knows what is human authored fact versus what is machine interpretation? Every inferred atom in that field is tagged with a confidence score and linked back to the exact line of pros it came from If the logic fails because of a bad inference you can trace it right back to the source Okay, so then layer seven is the zio unpacker which just turns that dictionary into the binary file Yep, it applies the field id's inserts the type markers and creates the final zl me be binary And the final layer layer eight is just one last check binary integrity check It immediately unpacks the file it just created to make sure it round trips perfectly before declaring it production ready That is an incredibly thorough pipeline, but there's one more piece the secret layer pass 2.5 the domain mapper This was a critical architectural decision The core pipeline pass one and pass two has to be universal it needs to work on any text That means you can't contaminate it with your game's specific weird lore the rules for real physics or dream events can't live in the universal extractor So how does the system connect a universal emotion like fear to a very specific game role like fear Near this object triggers a dream event. That's the job of the separate domain mapper pass 2.5 It lives downstream often inside the godot engine itself It takes the clean universal atoms from the main pipeline and uses them as triggers If pass two detects action drill manipulation near location sector gamma The domain mapper uses the game's lore database to translate that into a specific executable dream vent at runtime So it decouples the universal text comprehension from the specific game logic perfectly The extractor is a clean compiler for semantic facts The domain mapper is a spiralized layer of metaphysical rules It means the game designers can change their weird physics without ever having to touch the core nlp architecture Okay, so let's try to summarize this we've traced the whole flow From zw text through this eight layer eme key pipeline all the way to typesave zonb memory in godot It's it's a production ready system for turning any text into logic It really is and I think the breakthrough has really fallen to three main pillars. Okay. First, there's semantic compression with zw By just killing all the structural noise you let ai's reason about the world with way fewer tokens faster cheaper and more reliable Right with up to 47 character savings Then second you have architectural safety and speed with zonb The type safety and the field id mapping give you over 70 size reduction and lightning fast Predictable loading in godot and third and maybe most important is the data integrity from the eme pipeline That separation of explicit human fact from implicit machine interpretation means every piece of knowledge is verifiable and traceable Exactly, but as amazing as all that architecture is The documentation hints at one final ultimate stage The recursion layer this is where the whole thing stops being just an engine and starts becoming An autonomous creative entity a closed logic loop. What does that mean? It sounds like the system becoming self-aware in a way. It's the pursuit of creative autonomy The recursion layer is designed to take the engine's own output the new narrative fragments It generates at runtime and feed that back into the start of the pipeline So it reads its own writing it reads its own generated zw Recompiles it into zonb memory and then this is the key it validates that new memory against its own core rules The ap or architectural principles that govern the world's lore and physics So the engine generates a scene compiles it checks its own work against the rule book And if it passes it adds that scene to the official history of the world It's a self-correcting self-expanding process If the ai generates a scene that violates a core law of the universe The recursion layer catches it flags it and either corrects it or throws it out It prevents the world's logic from becoming corrupted by a bad generation So it's basically self editing its own ongoing novel ensuring it never contradicts itself precisely and that entire closed loop The ability to generate compile validate and store new memory all on its own that takes this architecture beyond just a game engine It positions it as a genuine self-correcting creative force And that that is the truly provocative future they're building and that level of creative autonomy all built on this foundation of Zw and zonb is a spectacular place to end. Thank you for joining us on this deep dive 